{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Notebook settings and imports.\"\"\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %flow mode reactive\n",
    "\n",
    "import os\n",
    "\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch as t\n",
    "\n",
    "from einops import asnumpy, einsum, rearrange, reduce, repeat, pack, parse_shape, unpack\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from jaxtyping import Float, Int\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import ticker as mticker\n",
    "from plotly import express as px\n",
    "from plotly import graph_objects as go\n",
    "from plotly import io as pio\n",
    "from rich import print as rprint\n",
    "from scipy.stats import describe, kstest, mannwhitneyu, sem\n",
    "from torch import nn, optim, Tensor\n",
    "from torch.nn import functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from toy_cis.models import CisConfig, Cis\n",
    "from toy_cis.plot import plot_weight_bars, plot_input_output_response\n",
    "from toy_cis.util import threshold_matrix, in_out_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set KMP_DUPLICATE_LIB_OK=TRUE to avoid MKL errors when plotting with mpl\"\"\"\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n",
    "plt.rcParams.update({\"font.size\": 14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_name='NVIDIA GeForce RTX 3090'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Set torch device.\"\"\"\n",
    "\n",
    "# device = t.device(\"cpu\")  # small toy models may be faster via cpu\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "device_name = t.cuda.get_device_name(0) if t.cuda.is_available() else \"cpu\"\n",
    "print(f\"{device_name=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transplant from res-embed-mlp to mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create functions for generating batches, loss, and training.\"\"\"\n",
    "\n",
    "def gen_batch(\n",
    "    model: Cis,\n",
    "    batch_sz: int,\n",
    "    sparsity: float | Float[Tensor, \"inst feat\"],\n",
    "    res_factor: float,\n",
    "    device: t.device\n",
    ") -> (\n",
    "    tuple[Float[Tensor, \"batch inst feat\"], Float[Tensor, \"batch inst feat\"]]\n",
    "):\n",
    "    \"\"\"Generates a batch of x, y data.\"\"\"\n",
    "    # Randomly generate features vals, and for each, randomly set which samples are non-zero\n",
    "    x = t.rand(batch_sz, model.cfg.n_instances, model.cfg.n_feat, device=device) * 2 - 1  # [-1, 1]\n",
    "    is_active = (\n",
    "        t.rand(batch_sz, model.cfg.n_instances, model.cfg.n_feat, device=device) < (1 - sparsity)\n",
    "    )\n",
    "    x *= is_active\n",
    "    return x, t.relu(x) + (res_factor * x)\n",
    "\n",
    "def loss_fn(y, y_true, i):\n",
    "    return reduce((y - y_true) ** 2 * i, \"batch inst feat -> \", \"mean\")\n",
    "\n",
    "def train(\n",
    "    model: Cis,\n",
    "    batch_sz: int,\n",
    "    feat_sparsity: float | Float[Tensor, \"inst feat\"],\n",
    "    feat_importance: float | Float[Tensor, \"inst feat\"],\n",
    "    res_factor: float,\n",
    "    loss_fn: Callable,\n",
    "    optimizer: optim.Optimizer,\n",
    "    n_steps: int,\n",
    "    logging_freq: int,\n",
    "    device: t.device\n",
    ") -> List[Float]:\n",
    "    \"\"\"Trains the model for `n_steps` steps, logging loss every `logging_freq` steps.\"\"\"    \n",
    "    losses = []\n",
    "\n",
    "    pbar = tqdm(range(n_steps), desc=\"Training\")\n",
    "    for step in pbar:\n",
    "        x, y_true = gen_batch(model, batch_sz, feat_sparsity, res_factor, device)\n",
    "        y = model(x, res_factor)\n",
    "        loss = loss_fn(y, y_true, feat_importance)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Log progress\n",
    "        if step % logging_freq == 0 or (step + 1 == n_steps):\n",
    "            losses.append(loss.item())\n",
    "            pbar.set_postfix({\"loss\": f\"{loss.item():.6f}\"})\n",
    "    \n",
    "    return losses\n",
    "\n",
    "@t.no_grad()\n",
    "def eval_model(\n",
    "    model: Cis, \n",
    "    batch_sz: int,\n",
    "    feat_sparsity: float | Float[Tensor, \"inst feat\"],\n",
    "    feat_importance: float | Float[Tensor, \"inst feat\"],\n",
    "    res_factor: float,\n",
    "    n_batches: int,\n",
    "    device: t.device\n",
    "):\n",
    "    losses = t.zeros(n_batches)\n",
    "    \n",
    "    for b in range(n_batches):\n",
    "        x, y_true = gen_batch(model, batch_sz, feat_sparsity, res_factor, device)\n",
    "        y = model(x, res_factor)\n",
    "        losses[b] = loss_fn(y, y_true, feat_importance)\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfdd122961e248289dfe3425e5bd8276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Create and train model.\"\"\"\n",
    "\n",
    "n_runs = 1\n",
    "min_loss = []\n",
    "layer_act_fns = [t.relu, lambda x: x]\n",
    "batch_sz = 1024\n",
    "n_feat = 100\n",
    "n_hidden = 50\n",
    "We_dim = 1000\n",
    "feat_sparsity = 0.99\n",
    "feat_importance = 1\n",
    "res_factor = 0.5\n",
    "n_steps = 20000\n",
    "logging_freq = n_steps // 10\n",
    "\n",
    "for run in range(n_runs):\n",
    "    # Create model\n",
    "    reluPlusX_res_embed_cfg = CisConfig(\n",
    "        n_instances=1,\n",
    "        n_feat=n_feat,\n",
    "        n_hidden=n_hidden,\n",
    "        act_fn=layer_act_fns,\n",
    "        b1=None,\n",
    "        b2=None,\n",
    "        skip_cnx=True,\n",
    "        We_and_Wu=True,\n",
    "        We_dim=We_dim,\n",
    "    )\n",
    "    reluPlusX_res_embed_cis = Cis(reluPlusX_res_embed_cfg, device=device)\n",
    "\n",
    "    # Train model\n",
    "    optimizer = t.optim.Adam(reluPlusX_res_embed_cis.parameters(), lr=5e-4)\n",
    "\n",
    "    losses = train(\n",
    "        reluPlusX_res_embed_cis,\n",
    "        batch_sz,\n",
    "        feat_sparsity,\n",
    "        feat_importance,\n",
    "        res_factor,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        n_steps,\n",
    "        logging_freq,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    # Eval model\n",
    "    eval_loss_adj = eval_model(\n",
    "        reluPlusX_res_embed_cis, \n",
    "        batch_sz=10000, \n",
    "        feat_sparsity=feat_sparsity,\n",
    "        feat_importance=feat_importance,\n",
    "        res_factor=res_factor,\n",
    "        n_batches=100,\n",
    "        device=device\n",
    "    ).mean().item() / (1 - feat_sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07169497548602514\n"
     ]
    }
   ],
   "source": [
    "print(eval_loss_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Transplant weights into mlp and eval.\"\"\"\n",
    "\n",
    "relu_cis_cfg = CisConfig(\n",
    "    n_instances=1,\n",
    "    n_feat=100,\n",
    "    n_hidden=50,\n",
    "    act_fn=layer_act_fns,\n",
    "    b1=None,\n",
    "    b2=None,\n",
    "    skip_cnx=True,\n",
    "    We_and_Wu=False,\n",
    ")\n",
    "\n",
    "relu_cis = Cis(relu_cis_cfg, device=device)\n",
    "\n",
    "transplant_w1 = einsum(\n",
    "    reluPlusX_res_embed_cis.We.squeeze(), \n",
    "    reluPlusX_res_embed_cis.W1.squeeze(), \n",
    "    \"emb feat, neur emb -> neur feat\"\n",
    ")\n",
    "transplant_w2 = einsum(\n",
    "    reluPlusX_res_embed_cis.W2.squeeze(),\n",
    "    reluPlusX_res_embed_cis.Wu.squeeze(), \n",
    "    \"emb neur, feat emb -> feat neur\"\n",
    ")\n",
    "\n",
    "relu_cis.W1.data[0] = transplant_w1\n",
    "relu_cis.W2.data[0] = transplant_w2\n",
    "\n",
    "eval_loss_adj = eval_model(\n",
    "    relu_cis, \n",
    "    batch_sz=10000, \n",
    "    feat_sparsity=feat_sparsity,\n",
    "    feat_importance=feat_importance,\n",
    "    res_factor=res_factor,\n",
    "    n_batches=100,\n",
    "    device=device\n",
    ").mean().item() / (1 - feat_sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08913635392673305\n"
     ]
    }
   ],
   "source": [
    "print(eval_loss_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08333333333333341\n"
     ]
    }
   ],
   "source": [
    "naive_loss = (n_feat - n_hidden) * (1 - feat_sparsity) / 6\n",
    "print(naive_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toy_cis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
