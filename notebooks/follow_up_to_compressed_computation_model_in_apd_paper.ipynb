{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology prerequisites\n",
    "\n",
    "*computation-in-superposition (CiS)*: <br>\n",
    "A model expressing CiS performs more computations than it has neurons, and takes advantage of superposition to perform better with sparser inputs.\n",
    "\n",
    "*compressed-computation (CC)*: <br>\n",
    "A model expressing CC performs more computations than it has neurons.\n",
    "\n",
    "*naive loss*: <br>\n",
    "The loss of a model that performs `n_neuron` computations perfectly. i.e. a baseline loss for the case where each neuron is performing one computation. A model performing CiS or CC must have loss lower than the naive loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In further investigations of the Toy Model of Compressed Computation (TMCC) in the APD paper, we find that the model may be doing something different to what the authors originally describe.\n",
    "\n",
    "Here are some points that support this:\n",
    "\n",
    "1. We find that both the embedding matrix and the residual stream are required in order for the model to perform below naive loss.\n",
    "\n",
    "2. Importantly, if we train a TMCC model called `EmbResMlp`, create a new model without the embed and unembed matrices called `ResMlp`, and transplant: <br>\n",
    "`einsum(EmbResMlp.We, EmbResMlp.W1, \"emb feat, neur emb -> neur feat\")` -> `ResMlp.W1` <br>\n",
    "`einsum(EmbResMlp.W2, EmbResMlp.Wu, \"emb neur, feat emb -> feat neur\")` -> `ResMlp.W2` <br>\n",
    "`ResMlp` never does better than naive loss.\n",
    "\n",
    "3. If we train `EmbResMlp` on L1-loss instead of L2-loss, it computes perfectly `n_neuron` features and outputs 0 for the remaining features.\n",
    "\n",
    "This suggests that `EmbResMlp` performs better than the naive loss not by performing meaningful computations on features, but instead by using the combination of the embedding matrix and residual stream to effectively cancel out noise or interference in the input space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Notebook settings and imports.\"\"\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %flow mode reactive\n",
    "\n",
    "import os\n",
    "\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch as t\n",
    "\n",
    "from einops import asnumpy, einsum, rearrange, reduce, repeat, pack, parse_shape, unpack\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from jaxtyping import Float, Int\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import ticker as mticker\n",
    "from plotly import express as px\n",
    "from plotly import graph_objects as go\n",
    "from plotly import io as pio\n",
    "from rich import print as rprint\n",
    "from scipy.stats import describe, kstest, mannwhitneyu, sem\n",
    "from torch import nn, optim, Tensor\n",
    "from torch.nn import functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from toy_cis.models import CisConfig, Cis\n",
    "from toy_cis.plot import plot_weight_bars, plot_input_output_response\n",
    "from toy_cis.util import threshold_matrix, in_out_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set KMP_DUPLICATE_LIB_OK=TRUE to avoid MKL errors when plotting with mpl\"\"\"\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n",
    "plt.rcParams.update({\"font.size\": 14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_name='NVIDIA GeForce RTX 3090'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Set torch device.\"\"\"\n",
    "\n",
    "# device = t.device(\"cpu\")  # small toy models may be faster via cpu\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "device_name = t.cuda.get_device_name(0) if t.cuda.is_available() else \"cpu\"\n",
    "print(f\"{device_name=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transplant from res-embed-mlp to mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create functions for generating batches, loss, and training.\"\"\"\n",
    "\n",
    "def gen_batch(\n",
    "    model: Cis,\n",
    "    batch_sz: int,\n",
    "    sparsity: float | Float[Tensor, \"inst feat\"],\n",
    "    res_factor: float,\n",
    "    device: t.device\n",
    ") -> (\n",
    "    tuple[Float[Tensor, \"batch inst feat\"], Float[Tensor, \"batch inst feat\"]]\n",
    "):\n",
    "    \"\"\"Generates a batch of x, y data.\"\"\"\n",
    "    # Randomly generate features vals, and for each, randomly set which samples are non-zero\n",
    "    x = t.rand(batch_sz, model.cfg.n_instances, model.cfg.n_feat, device=device) * 2 - 1  # [-1, 1]\n",
    "    is_active = (\n",
    "        t.rand(batch_sz, model.cfg.n_instances, model.cfg.n_feat, device=device) < (1 - sparsity)\n",
    "    )\n",
    "    x *= is_active\n",
    "    return x, t.relu(x) + (res_factor * x)\n",
    "\n",
    "def loss_fn(y, y_true, i):\n",
    "    return reduce((y - y_true) ** 2 * i, \"batch inst feat -> \", \"mean\")\n",
    "\n",
    "def train(\n",
    "    model: Cis,\n",
    "    batch_sz: int,\n",
    "    feat_sparsity: float | Float[Tensor, \"inst feat\"],\n",
    "    feat_importance: float | Float[Tensor, \"inst feat\"],\n",
    "    res_factor: float,\n",
    "    loss_fn: Callable,\n",
    "    optimizer: optim.Optimizer,\n",
    "    n_steps: int,\n",
    "    logging_freq: int,\n",
    "    device: t.device\n",
    ") -> List[Float]:\n",
    "    \"\"\"Trains the model for `n_steps` steps, logging loss every `logging_freq` steps.\"\"\"    \n",
    "    losses = []\n",
    "\n",
    "    pbar = tqdm(range(n_steps), desc=\"Training\")\n",
    "    for step in pbar:\n",
    "        x, y_true = gen_batch(model, batch_sz, feat_sparsity, res_factor, device)\n",
    "        y = model(x, res_factor)\n",
    "        loss = loss_fn(y, y_true, feat_importance)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Log progress\n",
    "        if step % logging_freq == 0 or (step + 1 == n_steps):\n",
    "            losses.append(loss.item())\n",
    "            pbar.set_postfix({\"loss\": f\"{loss.item():.6f}\"})\n",
    "    \n",
    "    return losses\n",
    "\n",
    "@t.no_grad()\n",
    "def eval_model(\n",
    "    model: Cis, \n",
    "    batch_sz: int,\n",
    "    feat_sparsity: float | Float[Tensor, \"inst feat\"],\n",
    "    feat_importance: float | Float[Tensor, \"inst feat\"],\n",
    "    res_factor: float,\n",
    "    n_batches: int,\n",
    "    device: t.device\n",
    "):\n",
    "    losses = t.zeros(n_batches)\n",
    "    \n",
    "    for b in range(n_batches):\n",
    "        x, y_true = gen_batch(model, batch_sz, feat_sparsity, res_factor, device)\n",
    "        y = model(x, res_factor)\n",
    "        losses[b] = loss_fn(y, y_true, feat_importance)\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538a059137754e0bbf17d423f8fb3fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Create and train model.\"\"\"\n",
    "\n",
    "n_runs = 1\n",
    "min_loss = []\n",
    "layer_act_fns = [t.relu, lambda x: x]\n",
    "batch_sz = 1024\n",
    "n_feat = 100\n",
    "n_hidden = 50\n",
    "We_dim = 1000\n",
    "feat_sparsity = 0.99\n",
    "feat_importance = 1\n",
    "res_factor = 0\n",
    "n_steps = 20000\n",
    "logging_freq = n_steps // 10\n",
    "\n",
    "for run in range(n_runs):\n",
    "    # Create model\n",
    "    reluPlusX_res_embed_cfg = CisConfig(\n",
    "        n_instances=1,\n",
    "        n_feat=n_feat,\n",
    "        n_hidden=n_hidden,\n",
    "        act_fn=layer_act_fns,\n",
    "        b1=None,\n",
    "        b2=None,\n",
    "        skip_cnx=False,\n",
    "        We_and_Wu=True,\n",
    "        We_dim=We_dim,\n",
    "    )\n",
    "    reluPlusX_res_embed_cis = Cis(reluPlusX_res_embed_cfg, device=device)\n",
    "\n",
    "    # Train model\n",
    "    optimizer = t.optim.Adam(reluPlusX_res_embed_cis.parameters(), lr=5e-4)\n",
    "\n",
    "    losses = train(\n",
    "        reluPlusX_res_embed_cis,\n",
    "        batch_sz,\n",
    "        feat_sparsity,\n",
    "        feat_importance,\n",
    "        res_factor,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        n_steps,\n",
    "        logging_freq,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    # Eval model\n",
    "    eval_loss_adj = eval_model(\n",
    "        reluPlusX_res_embed_cis, \n",
    "        batch_sz=10000, \n",
    "        feat_sparsity=feat_sparsity,\n",
    "        feat_importance=feat_importance,\n",
    "        res_factor=res_factor,\n",
    "        n_batches=100,\n",
    "        device=device\n",
    "    ).mean().item() / (1 - feat_sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08319638436660164\n"
     ]
    }
   ],
   "source": [
    "print(eval_loss_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Transplant weights into mlp and eval.\"\"\"\n",
    "\n",
    "relu_cis_cfg = CisConfig(\n",
    "    n_instances=1,\n",
    "    n_feat=100,\n",
    "    n_hidden=50,\n",
    "    act_fn=layer_act_fns,\n",
    "    b1=None,\n",
    "    b2=None,\n",
    "    skip_cnx=True,\n",
    "    We_and_Wu=False,\n",
    ")\n",
    "\n",
    "relu_cis = Cis(relu_cis_cfg, device=device)\n",
    "\n",
    "transplant_w1 = einsum(\n",
    "    reluPlusX_res_embed_cis.We.squeeze(), \n",
    "    reluPlusX_res_embed_cis.W1.squeeze(), \n",
    "    \"emb feat, neur emb -> neur feat\"\n",
    ")\n",
    "transplant_w2 = einsum(\n",
    "    reluPlusX_res_embed_cis.W2.squeeze(),\n",
    "    reluPlusX_res_embed_cis.Wu.squeeze(), \n",
    "    \"emb neur, feat emb -> feat neur\"\n",
    ")\n",
    "\n",
    "relu_cis.W1.data[0] = transplant_w1\n",
    "relu_cis.W2.data[0] = transplant_w2\n",
    "\n",
    "eval_loss_adj = eval_model(\n",
    "    relu_cis, \n",
    "    batch_sz=10000, \n",
    "    feat_sparsity=feat_sparsity,\n",
    "    feat_importance=feat_importance,\n",
    "    res_factor=res_factor,\n",
    "    n_batches=100,\n",
    "    device=device\n",
    ").mean().item() / (1 - feat_sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08913635392673305\n"
     ]
    }
   ],
   "source": [
    "print(eval_loss_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08333333333333341\n"
     ]
    }
   ],
   "source": [
    "naive_loss = (n_feat - n_hidden) * (1 - feat_sparsity) / 6\n",
    "print(naive_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toy_cis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
